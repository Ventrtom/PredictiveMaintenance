# üß† Project Snapshot

Tento soubor obsahuje strukturu projektu a obsah jednotliv√Ωch soubor≈Ø pro pou≈æit√≠ s AI asistenty.

## üìÇ Struktura projektu

```
PredictiveMaintenance/
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ config
‚îÇ   ‚îî‚îÄ‚îÄ db_config.py
‚îú‚îÄ‚îÄ data
‚îÇ   ‚îú‚îÄ‚îÄ data_loader.py
‚îÇ   ‚îî‚îÄ‚îÄ database_connection.py
‚îú‚îÄ‚îÄ main.py
‚îú‚îÄ‚îÄ models
‚îÇ   ‚îú‚îÄ‚îÄ base_model.py
‚îÇ   ‚îî‚îÄ‚îÄ random_forest.py
‚îú‚îÄ‚îÄ notebooks
‚îÇ   ‚îú‚îÄ‚îÄ data_connection_test.ipynb
‚îÇ   ‚îî‚îÄ‚îÄ data_filtering_test.ipynb
‚îú‚îÄ‚îÄ requirements.txt
‚îî‚îÄ‚îÄ utils
    ‚îú‚îÄ‚îÄ adaptive_logic.py
    ‚îú‚îÄ‚îÄ preprocessing.py
    ‚îú‚îÄ‚îÄ preprocessing2.py
    ‚îî‚îÄ‚îÄ visualization.py
```

## üìÑ Obsahy soubor≈Ø


---

### `main.py`

```python

```


---

### `README.md`

```python
Projekt je urƒçen pro vytvo≈ôen√≠ prediktivn√≠ho modelu √∫dr≈æby na z√°kladƒõ dat sb√≠ran√Ωch p≈ô√≠mo z v√Ωrobn√≠ch stroj≈Ø. Nasb√≠ran√° data v datab√°zi jsou z√≠sk√°na z re√°ln√©ho provozu.


Jak pou≈æ√≠vat tento project:
Aktivace venv:
 .venv\Scripts\activate

Instalace knihoven z requirements.txt:
 pip install -r requirements.txt


Stav projektu a n√°vrh dal≈°√≠ho postupu
Souƒçasn√Ω stav
Projekt je napojen na Oracle datab√°zi pomoc√≠ SQLAlchemy. K dispozici jsou funkce pro naƒçten√≠:

senzorov√Ωch dat z tabulky FC_OBJECT_DATA

KPI definic z tabulky FC_KPI_DEFINITION

informac√≠ o za≈ô√≠zen√≠ch z tabulky EQUIPMENT

z√°znam≈Ø o √∫dr≈æbƒõ z tabulky MAINTENANCE_ORDER

Data ze stroj≈Ø jsou ji≈æ p≈ôedzpracov√°v√°na pomoc√≠ adaptivn√≠ch strategi√≠ (preprocessing.py - OBSOLETE, lze smazat, preprocessing2.py). Vizualizaƒçn√≠ n√°stroje (visualization.py) umo≈æ≈àuj√≠ sledovat trendy, rolling metriky, korelace a chybƒõj√≠c√≠ hodnoty.

Identifikovan√© mezery a pot≈ôeby
1. Propojen√≠ senzorov√Ωch dat se z√°znamy √∫dr≈æby
Maintenance order reprezentuje z√°sah prov√°dƒõn√Ω nad konkr√©tn√≠m za≈ô√≠zen√≠m (EQUIPMENT). Ka≈æd√© za≈ô√≠zen√≠ je sv√°z√°no s FC_OBJECT, a senzorov√° data jsou v√°z√°na pr√°vƒõ na id_fc_object. Pro jakoukoliv anal√Ωzu a predikci je nutn√© propojit senzorov√° data a z√°znamy √∫dr≈æby skrz tyto vazby.

Akce:

Ovƒõ≈ôit a implementovat propojen√≠ MAINTENANCE_ORDER ‚Üí EQUIPMENT ‚Üí FC_OBJECT ‚Üí FC_OBJECT_DATA.
Zat√≠m n√°m v DB chyb√≠ data o vazbƒõ mezi EQUIPMENT a FC_OBJECT kde je pouze jedin√Ω object ale equipment≈Ø je hned nƒõkolik. Equipmenty maj√≠ ve slouci EQUIPMENT_NUMBER uveden√© k√≥dov√© oznaƒçen√≠ kdy teoreticky p≈ôes prefix m≈Ø≈æeme vytvo≈ôit vazbu equipmentu a konkr√©tn√≠ho KPI kter√© zd√° se pou≈æ√≠v√° stejn√Ω prefix? Ale to u≈æ je specifick√Ω case pro tento konkr√©tn√≠ dataset a my pot≈ôebujeme vytvo≈ôit univerz√°ln√≠ funkci. Proto i kdybychom mƒõli pozmƒõnit tato aktu√°ln√≠ data v datab√°zi tak, ≈æe nebudou v≈°echna KPI pod jedin√Ωm objectem ale zalo≈æ√≠me FC_OBJECT pro ka≈æd√Ω EQUIPMENT a spr√°vnƒõ namapujeme FC_KPI_DEFINITION a FC_OBJECT a EQUIPMENT. T√≠m bychom mƒõli z√≠skat spr√°vnou podobu datasetu.

Vytvo≈ôit funkci fetch_maintenance_with_object_ids().

2. Rozli≈°en√≠ typ≈Ø √∫dr≈æby
Pro predikƒçn√≠ √∫ƒçely n√°s zaj√≠maj√≠ p≈ôedev≈°√≠m breakdown/corrective typy poruch. Preventivn√≠ √∫dr≈æby je t≈ôeba ze znaƒçen√≠ poruch odfiltrovat.

Akce:

Zajistit rozli≈°en√≠ typ≈Ø v r√°mci dat z MAINTENANCE_ORDER (podle sloupce type ID_LISTING_VALUE=1 kde 1 reprezentuje typ breakdown, 3 corrective a 101 preventive).

P≈ôidat filtr poruch podle typu.

3. ƒåasov√° prodleva mezi poruchou a vytvo≈ôen√≠m/zah√°jen√≠m √∫dr≈æby
V re√°ln√©m provozu ƒçasto existuje prodleva mezi skuteƒçn√Ωm vznikem poruchy a vytvo≈ôen√≠m/zah√°jen√≠m MAINTENANCE_ORDER. Pro predikci je nejvhodnƒõj≈°√≠ orientovat se podle ƒçasu vzniku nebo vytvo≈ôen√≠ z√°znamu, ne podle jeho zah√°jen√≠.

Akce:

Pracovat s created_at jako s aproximac√≠ ƒçasu poruchy.

Vytvo≈ôit ƒçasov√© okno (nap≈ô. 60‚Äì180 minut p≈ôed created_at) a oznaƒçit data v tomto oknƒõ jako ji≈æ potenci√°lnƒõ odpov√≠daj√≠c√≠ poru≈°e tedy ≈æe bƒõhem t√©to doby u≈æ porucha prob√≠h√° a analyzovat p≈ôev√°≈ænƒõ je≈°tƒõ d≈ô√≠vƒõj≈°√≠ data.

Doporuƒçen√Ω dal≈°√≠ postup
F√°ze 1 ‚Äì Slouƒçen√≠ dat
Implementovat funkci pro propojen√≠ z√°znam≈Ø √∫dr≈æby se senzory na z√°kladƒõ equipment_id a id_fc_object.

Vytvo≈ôit funkci create_labeled_dataset(sensor_df, maintenance_df), kter√° vytvo≈ô√≠ tr√©novac√≠ dataset se sloupci featur a bin√°rn√≠m labelem oznaƒçuj√≠c√≠m p≈ô√≠tomnost poruchy.

F√°ze 2 ‚Äì Modelov√°n√≠
Pou≈æ√≠t preprocess_sensor_data2 pro generov√°n√≠ vstupn√≠ch featur.

Vytvo≈ôit datov√© matice X a y pro tr√©nink klasifikaƒçn√≠ho modelu.

Implementovat z√°kladn√≠ model (nap≈ô. RandomForest) v models/random_forest.py.

F√°ze 3 ‚Äì Evaluace a interpretace
Vyhodnotit model pomoc√≠ metrik jako p≈ôesnost, recall a F1-score.

Vizualizovat v√Ωznam jednotliv√Ωch featur (feature importance).

Vytvo≈ôit ROC k≈ôivku a matici z√°mƒõn.

Doporuƒçen√≠ k implementaci
Vytvo≈ôit nov√Ω skript labeled_dataset.py v utils/, kter√Ω bude:

Spojovat data senzor≈Ø a √∫dr≈æby.

Generovat bin√°rn√≠ label pro p≈ô√≠tomnost poruchy.

P≈ôidat notebook notebooks/model_training.ipynb pro experimenty:

Naƒçten√≠ dat.

Preprocessing.

Tr√©nov√°n√≠ modelu.

Vizualizace v√Ωsledk≈Ø.
```


---

### `requirements.txt`

```python
# Pr√°ce s datab√°z√≠ Oracle
oracledb>=1.3  # ofici√°ln√≠ knihovna pro Oracle (n√°hrada za cx_Oracle)
SQLAlchemy>=2.0

# Datov√© modelov√°n√≠ a validace
pydantic>=2.5

# Z√°kladn√≠ pr√°ce s daty
pandas>=2.2
numpy>=1.26

# Logov√°n√≠ a ladƒõn√≠
loguru>=0.7

# .env konfigurace
python-dotenv>=1.0

# vizualizace
matplotlib
seaborn

# interaktivn√≠ testov√°n√≠
jupyterlab

# ML knihovny
scikit-learn
```


---

### `config\db_config.py`

```python

```


---

### `data\database_connection.py`

```python
import os
import pandas as pd
from dotenv import load_dotenv
from sqlalchemy import create_engine

# Naƒçten√≠ promƒõnn√Ωch z .env souboru
load_dotenv()

ORACLE_USER = os.getenv("ORACLE_USER")
ORACLE_PASSWORD = os.getenv("ORACLE_PASSWORD")
ORACLE_HOST = os.getenv("ORACLE_HOST")
ORACLE_PORT = os.getenv("ORACLE_PORT")
ORACLE_SERVICE = os.getenv("ORACLE_SERVICE")

# Sestaven√≠ SQLAlchemy URI
DB_URI = f"oracle+oracledb://{ORACLE_USER}:{ORACLE_PASSWORD}@{ORACLE_HOST}:{ORACLE_PORT}/{ORACLE_SERVICE}"

def get_engine():
    """Vrac√≠ SQLAlchemy engine pro p≈ôipojen√≠ k datab√°zi."""
    try:
        engine = create_engine(DB_URI, echo=False, future=True)
        print("‚úÖ SQLAlchemy engine vytvo≈ôen.")
        return engine
    except Exception as e:
        print("‚ùå Chyba p≈ôi vytv√°≈ôen√≠ SQLAlchemy engine:", e)
        return None
    
def test_db_connection():
    engine = get_engine()
    if engine:
        try:
            df = pd.read_sql("SELECT * FROM dual", engine)
            print("üì¶ Testovac√≠ v√Ωstup:", df)
        except Exception as e:
            print("‚ùå Test spojen√≠ selhal:", e)

```


---

### `data\data_loader.py`

```python
import pandas as pd
from data.database_connection import get_engine

def fetch_equipment_data():
    engine = get_engine()
    if engine:
        return pd.read_sql("SELECT * FROM EQUIPMENT", engine)

def fetch_kpi_definitions():
    engine = get_engine()
    if engine:
        return pd.read_sql("SELECT * FROM FC_KPI_DEFINITION", engine)

def fetch_sensor_data():
    engine = get_engine()
    if engine:
        return pd.read_sql("SELECT * FROM FC_OBJECT_DATA", engine)

def fetch_maintenance_orders():
    engine = get_engine()
    if engine:
        return pd.read_sql("SELECT * FROM MAINTENANCE_ORDER", engine)
    
# Rychl√© testovac√≠ n√°hledy
def preview_equipment_data():
    engine = get_engine()
    if engine:
        return pd.read_sql("SELECT * FROM EQUIPMENT FETCH FIRST 8 ROWS ONLY", engine)

def preview_kpi_definitions():
    engine = get_engine()
    if engine:
        return pd.read_sql("SELECT * FROM FC_KPI_DEFINITION FETCH FIRST 8 ROWS ONLY", engine)

def preview_sensor_data():
    engine = get_engine()
    if engine:
        return pd.read_sql("SELECT * FROM FC_OBJECT_DATA FETCH FIRST 8 ROWS ONLY", engine)

def preview_maintenance_orders():
    engine = get_engine()
    if engine:
        return pd.read_sql("SELECT * FROM MAINTENANCE_ORDER FETCH FIRST 8 ROWS ONLY", engine)
    
def fetch_sensor_data_filtered(object_id: int = None, kpi_ids: list = None, start_time: str = None, end_time: str = None):
    import pandas as pd
    from sqlalchemy import text

    engine = get_engine()
    if not engine:
        return None

    # Z√°kladn√≠ dotaz
    query = "SELECT * FROM FC_OBJECT_DATA WHERE 1=1"

    # Filtrov√°n√≠ dle objektu
    if object_id is not None:
        query += f" AND id_fc_object = {object_id}"

        # Pokud nejsou kpi_ids, dynamicky je z√≠sk√°me z datab√°ze
        if not kpi_ids:
            kpi_query = f"SELECT DISTINCT id_fc_kpi_definition FROM FC_OBJECT_DATA WHERE id_fc_object = {object_id}"
            kpi_ids_df = pd.read_sql(text(kpi_query), engine)
            kpi_ids = kpi_ids_df['id_fc_kpi_definition'].tolist()

    # Filtrov√°n√≠ dle KPI
    if kpi_ids:
        formatted_ids = ", ".join(str(kpi) for kpi in kpi_ids)
        query += f" AND id_fc_kpi_definition IN ({formatted_ids})"

    # Filtrov√°n√≠ dle ƒçasov√©ho rozsahu
    if start_time and end_time:
        query += (
            f" AND data_timestamp BETWEEN "
            f"TO_TIMESTAMP('{start_time}', 'YYYY-MM-DD HH24:MI:SS') AND "
            f"TO_TIMESTAMP('{end_time}', 'YYYY-MM-DD HH24:MI:SS')"
        )

    return pd.read_sql(text(query), engine)




def get_unique_equipment_ids():
    """Vrac√≠ unik√°tn√≠ ID za≈ô√≠zen√≠ z tabulky EQUIPMENT."""
    engine = get_engine()
    if engine:
        query = "SELECT DISTINCT ID AS equipment_id FROM EQUIPMENT ORDER BY ID"
        df = pd.read_sql(query, engine)
        return df['equipment_id'].dropna().tolist()
    return []


def get_unique_kpi_ids():
    """Vrac√≠ unik√°tn√≠ ID KPI z tabulky FC_OBJECT_DATA."""
    engine = get_engine()
    if engine:
        query = "SELECT DISTINCT ID_FC_KPI_DEFINITION AS kpi_id FROM FC_OBJECT_DATA ORDER BY ID_FC_KPI_DEFINITION"
        df = pd.read_sql(query, engine)
        return df['kpi_id'].dropna().tolist()
    return []

def get_unique_object_ids():
    """Vrac√≠ unik√°tn√≠ id_fc_object z tabulky FC_OBJECT_DATA, pokud existuje."""
    engine = get_engine()
    if engine:
        query = "SELECT DISTINCT id_fc_object FROM FC_OBJECT_DATA ORDER BY id_fc_object"
        df = pd.read_sql(query, engine)
        return pd.read_sql(query, engine)['id_fc_object'].dropna().tolist()
    return []
```


---

### `models\base_model.py`

```python
from pydantic import BaseModel
from typing import Optional
from datetime import datetime

class Equipment(BaseModel):
    id: int
    equipment_number: str
    description: Optional[str]
    serial_number: Optional[str]

class KPIDefinition(BaseModel):
    id: int
    name: str
    description: Optional[str]
    unit: Optional[str]

class SensorReading(BaseModel):
    id: int
    equipment_id: int
    kpi_id: int
    value: float
    timestamp: datetime

class MaintenanceOrder(BaseModel):
    id: int
    equipment_id: int
    type: str
    created_at: datetime
    description: Optional[str]
```


---

### `models\random_forest.py`

```python

```


---

### `notebooks\data_connection_test.ipynb`

```python
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4cbe4e0b",
   "metadata": {},
   "source": [
    "# üîç Test naƒç√≠t√°n√≠ dat z Oracle DB\n",
    "Tento notebook ovƒõ≈ôuje, ≈æe funguje p≈ôipojen√≠ k datab√°zi a ≈æe lze naƒç√≠st z√°znamy ze v≈°ech kl√≠ƒçov√Ωch tabulek."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67531868",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "from data.data_loader import (\n",
    "    preview_equipment_data,\n",
    "    preview_kpi_definitions,\n",
    "    fetch_kpi_definitions,\n",
    "    preview_sensor_data,\n",
    "    preview_maintenance_orders\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c5631e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìÑ EQUIPMENT ‚Äì informace o za≈ô√≠zen√≠ch\n",
    "equipment_df = preview_equipment_data()\n",
    "equipment_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde49f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìÑ FC_KPI_DEFINITION ‚Äì seznam mƒõ≈ôen√Ωch veliƒçin\n",
    "#kpi_df = preview_kpi_definitions()\n",
    "kpi_df = fetch_kpi_definitions()\n",
    "kpi_df.head(50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ed11c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìÑ FC_OBJECT_DATA ‚Äì historick√° senzorov√° data\n",
    "sensor_df = preview_sensor_data()\n",
    "sensor_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9d9056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìÑ MAINTENANCE_ORDER ‚Äì z√°znamy o √∫dr≈æbƒõ\n",
    "maintenance_df = preview_maintenance_orders()\n",
    "maintenance_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

```


---

### `notebooks\data_filtering_test.ipynb`

```python
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb6f67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß† Predictive Maintenance ‚Äì Test naƒç√≠t√°n√≠ dat s filtrem\n",
    "# üì¶ Importy:\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "from data.database_connection import get_engine\n",
    "from data.data_loader import (\n",
    "    get_unique_equipment_ids,\n",
    "    get_unique_kpi_ids,\n",
    "    get_unique_object_ids,\n",
    "    fetch_sensor_data_filtered\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b012ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Z√≠sk√°n√≠ dostupn√Ωch hodnot ---\n",
    "\n",
    "print(\"üîé Naƒç√≠t√°m dostupn√© hodnoty z datab√°ze...\")\n",
    "\n",
    "equipment_ids = get_unique_equipment_ids()\n",
    "kpi_ids = get_unique_kpi_ids()\n",
    "object_ids = get_unique_object_ids()\n",
    "\n",
    "print(f\"‚úÖ {len(equipment_ids)} unik√°tn√≠ch equipment_id\")\n",
    "print(f\"‚úÖ {len(kpi_ids)} unik√°tn√≠ch kpi_id\")\n",
    "print(f\"‚úÖ {len(object_ids)} unik√°tn√≠ch object_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef94b4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Ruƒçn√≠ v√Ωbƒõr hodnot pro testov√°n√≠ ---\n",
    "\n",
    "# ZDE Mƒö≈á PARAMETRY PRO TEST\n",
    "selected_object_id = 1\n",
    "selected_kpi_id = None # nebo konkr√©tn√≠ KPI stylem [1,2,3] \n",
    "start_time = \"2025-01-01 00:00:00\"\n",
    "end_time = \"2025-03-01 00:00:00\"\n",
    "\n",
    "print(f\"\\nüéØ Vybran√© parametry:\\n- FC OBJECT ID: {selected_object_id}\\n- KPI ID: {selected_kpi_id}\\n- Obdob√≠: {start_time} ‚Äì {end_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8470d388",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Filtrov√°n√≠ dat ---\n",
    "filtered_df = fetch_sensor_data_filtered(\n",
    "    object_id=selected_object_id,\n",
    "    kpi_ids=selected_kpi_id,\n",
    "    start_time=start_time,\n",
    "    end_time=end_time\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27df6407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. N√°hled v√Ωsledku ---\n",
    "print(f\"\\nüìä Naƒçteno {len(filtered_df)} ≈ô√°dk≈Ø.\")\n",
    "filtered_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c07619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# P≈ôedzpracov√°n√≠ vy≈æaduje sloupce: timestamp, object_id, kpi_id, value\n",
    "\n",
    "df = filtered_df.rename(columns={\n",
    "    'data_timestamp': 'timestamp',\n",
    "    'id_fc_object': 'object_id',\n",
    "    'id_fc_kpi_definition': 'kpi_id',\n",
    "    'value': 'value'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c78c4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spu≈°tƒõn√≠ p≈ôedzpracov√°n√≠\n",
    "from utils.preprocessing import preprocess_sensor_data\n",
    "from utils.preprocessing2 import preprocess_sensor_data2\n",
    "\n",
    "processed = preprocess_sensor_data2(df, impute=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d597748f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vezmeme prvn√≠ stroj (object_id) a zobraz√≠me jeho dataframe\n",
    "if processed:\n",
    "    first_object_id = list(processed.keys())[0]\n",
    "    print(f\"üîç V√Ωstup pro object_id: {first_object_id}\")\n",
    "    display(processed[first_object_id].head(5))\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Nebyla nalezena ≈æ√°dn√° data po zpracov√°n√≠.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17887340",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.visualization import (\n",
    "    plot_kpi_raw_trends,\n",
    "    plot_kpi_rolling,\n",
    "    plot_correlation_heatmap,\n",
    "    plot_feature_distributions,\n",
    "    plot_missing_data_pattern\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22670ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trendy pro KPI\n",
    "plot_kpi_raw_trends(\n",
    "    df=processed,            # Vstupn√≠ data ‚Äì buƒè slovn√≠k {object_id: DataFrame}, nebo p≈ô√≠mo DataFrame\n",
    "    object_id=1,             # ID objektu ‚Äì pou≈æije se pouze pokud je df slovn√≠k\n",
    "\n",
    "    kpi_ids=None,            # Seznam KPI, kter√© chce≈° vykreslit (nap≈ô. [7, 8, 23])\n",
    "                             # Pokud None, zobraz√≠ se v≈°echny ƒç√≠seln√© sloupce (raw i engineered)\n",
    "\n",
    "    feature_types=[          # Voliteln√© z√∫≈æen√≠ podle typu feature:\n",
    "        'raw'              \n",
    "        #   'raw' Origin√°ln√≠ KPI hodnoty (nap≈ô. '7', '8', '23')             \n",
    "        #   'mean' Rolling pr≈Ømƒõry (nap≈ô. 'mean_6')               \n",
    "        #   'std' Rolling smƒõrodatn√© odchylky (nap≈ô. 'std_3')       \n",
    "        #   'diff1' Prvn√≠ diference\n",
    "        #   'diff2' Druh√° diference        \n",
    "        #   'pct_change' Procentu√°ln√≠ zmƒõna        \n",
    "        #   'time_since' Speci√°ln√≠ sloupec 'time_since_last'\n",
    "        #   'rolling' Z√°stupn√Ω typ ‚Äì zahrne mean + std dohromady\n",
    "    ],\n",
    "\n",
    "    agg_freq='1h',           # Agregaƒçn√≠ frekvence (resampling), nap≈ô. '1min', '15t', '1h'\n",
    "                             # Pou≈æije se na ƒçasovou osu ‚Äì nap≈ô. 1h = pr≈Ømƒõrov√°n√≠ po hodin√°ch\n",
    "\n",
    "    highlight_missing=True,   # Zv√Ωraznit chybƒõj√≠c√≠ hodnoty ƒçerven√Ωmi znaƒçkami (nap≈ô. kde je NaN)\n",
    "    start_time=None,   # Poƒç√°teƒçn√≠ ƒçasov√Ω filtr buƒèto nechat \"start_time\" nebo konkr√©tn√≠ ƒças (nap≈ô. \"2025-01-01 00:00:00\")\n",
    "    end_time=None       # Koncov√Ω ƒçasov√Ω filtr buƒèto nechat \"end_time\" nebo konkr√©tn√≠ ƒças (nap≈ô. \"2025-03-01 00:00:00\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e40cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rolling mean s oknem 6 vzork≈Ø\n",
    "plot_kpi_rolling(\n",
    "    df=processed,                # Dict nebo DataFrame\n",
    "    object_id=1,                 # ID objektu (stroje)\n",
    "    kpi_ids=[1],                # Seznam KPI (nap≈ô. [23, 24]) nebo None = v≈°echny KPI\n",
    "    window=6,                    # Rolling okno (poƒçet ƒçasov√Ωch krok≈Ø), nap≈ô. 6\n",
    "    method='mean',                # Typ v√Ωpoƒçtu: 'mean', 'std', 'max', 'min', ...\n",
    "    start_time=start_time,       # Poƒç√°teƒçn√≠ ƒçasov√Ω filtr\n",
    "    end_time=end_time            # Koncov√Ω ƒçasov√Ω filtr\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac41a839",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_correlation_heatmap(\n",
    "    df=processed,            # Vstupn√≠ data ‚Äì buƒè DataFrame nebo dict ve form√°tu {object_id: DataFrame}\n",
    "    object_id=1,             # ID objektu ‚Äì pou≈æije se jako kl√≠ƒç pro v√Ωbƒõr ze slovn√≠ku (pokud je df typu dict)\n",
    "\n",
    "    kpi_ids=None,            # Voliteln√Ω v√Ωbƒõr konkr√©tn√≠ch KPI podle ID ‚Äì nap≈ô. [7, 8, 23]\n",
    "                             # Pokud None, pou≈æij√≠ se v≈°echny KPI (dle v√Ωbƒõru feature types n√≠≈æe)\n",
    "\n",
    "    feature_types=[          # V√Ωbƒõr typ≈Ø sloupc≈Ø (feature), kter√© se maj√≠ zahrnout do korelaƒçn√≠ matice:\n",
    "        'raw'           \n",
    "        #   'raw' Pouze p≈Øvodn√≠ KPI sloupce, nap≈ô. '7', '8', '23'\n",
    "        #   'mean' Rolling pr≈Ømƒõr, nap≈ô. '7_mean_6'\n",
    "        #   'std' Rolling smƒõrodatn√° odchylka, nap≈ô. '7_std_6'\n",
    "        #   'diff1' Prvn√≠ diference, nap≈ô. '7_diff1'\n",
    "        #   'diff2' Druh√° diference, nap≈ô. '7_diff2'\n",
    "        #   'pct_change' Procentu√°ln√≠ zmƒõna, nap≈ô. '7_pct_change'\n",
    "        #   'time_since' Speci√°ln√≠ feature nap≈ô. 'time_since_last'\n",
    "        #   'rolling' Zahrne v≈°echny rolling features (mean_, std_), alias pro pohodl√≠\n",
    "    ],\n",
    "    start_time=start_time,       # Poƒç√°teƒçn√≠ ƒçasov√Ω filtr\n",
    "    end_time=end_time            # Koncov√Ω ƒçasov√Ω filtr\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e60be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogramy dat\n",
    "plot_feature_distributions(\n",
    "    df=processed,              # Dict nebo DataFrame\n",
    "    object_id=1,               # ID objektu\n",
    "    kpi_ids=[1],              # Konkr√©tn√≠ KPI (nap≈ô. [8, 23, 25]); None = v≈°echny\n",
    "    raw=False,                 # True = pou≈æ√≠t neskalovan√° data (pouze origin√°ln√≠ KPI)\n",
    "    log=False,                  # True = logaritmick√° osa pro histogram\n",
    "    start_time=start_time,       # Poƒç√°teƒçn√≠ ƒçasov√Ω filtr\n",
    "    end_time=end_time            # Koncov√Ω ƒçasov√Ω filtr\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab9fd29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vizualizace chybƒõj√≠c√≠ch dat\n",
    "plot_missing_data_pattern(\n",
    "    df=processed,              # Dict nebo DataFrame\n",
    "    object_id=1,               # ID objektu\n",
    "    n_rows=500,                 # Poƒçet ƒçasov√Ωch ≈ô√°dk≈Ø k zobrazen√≠ (nap≈ô. 500 = prvn√≠ch 500 timestamps)\n",
    "    start_time=start_time,       # Poƒç√°teƒçn√≠ ƒçasov√Ω filtr\n",
    "    end_time=end_time            # Koncov√Ω ƒçasov√Ω filtr\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

```


---

### `utils\adaptive_logic.py`

```python
import pandas as pd
import numpy as np
from typing import Tuple, Dict

def infer_base_frequency(series: pd.Series) -> str:
    """
    Odhadne vhodnou c√≠lovou frekvenci na z√°kladƒõ ƒçasov√Ωch rozd√≠l≈Ø.
    """
    deltas = series.dropna().sort_values().diff().dropna().dt.total_seconds()
    if deltas.empty:
        return '1H'  # default

    median_delta = np.median(deltas)
    if median_delta < 60:
        return '1T'
    elif median_delta < 300:
        return '5T'
    elif median_delta < 1800:
        return '15T'
    elif median_delta < 7200:
        return '1H'
    else:
        return '3H'

def infer_best_aggregation(series: pd.Series) -> str:
    """
    Vybere vhodnou agregaƒçn√≠ metodu na z√°kladƒõ variability dat.
    """
    if series.nunique() == 1:
        return 'last'
    std_dev = series.std()
    if std_dev < 0.1 * abs(series.mean()):
        return 'mean'
    elif std_dev < 0.3 * abs(series.mean()):
        return 'median'
    elif series.skew() > 2:
        return 'max'
    else:
        return 'mean'

def choose_resample_strategy(df: pd.DataFrame) -> Dict[str, Dict[str, str]]:
    """
    Vybere vhodnou resample frekvenci a agregaƒçn√≠ metodu pro ka≈æd√Ω KPI.

    Returns:
        {kpi_id: {'freq': '15T', 'agg': 'mean'}}
    """
    strategies = {}
    grouped = df.groupby("kpi_id")
    for kpi_id, group in grouped:
        freq = infer_base_frequency(group["timestamp"])
        agg = infer_best_aggregation(group["value"])
        strategies[kpi_id] = {"freq": freq, "agg": agg}
    return strategies

def choose_imputation_method(series: pd.Series) -> str:
    """
    Rozhodne, jakou metodu imputace pou≈æ√≠t.
    """
    missing_ratio = series.isnull().sum() / len(series)
    if missing_ratio < 0.02:
        return 'none'
    elif missing_ratio < 0.1:
        return 'interpolate'
    elif series.ffill().nunique() <= 3:
        return 'mean'
    else:
        return 'ffill'

def choose_scaling_method(series: pd.Series) -> str:
    """
    Rozhodne, jak√Ωm zp≈Øsobem normalizovat data.
    """
    if series.max() - series.min() < 1e-3:
        return 'none'
    elif series.skew() > 2:
        return 'robust'
    elif abs(series.mean()) > 10 * series.std():
        return 'minmax'
    else:
        return 'zscore'

```


---

### `utils\preprocessing.py`

```python
# preprocessing.py

import pandas as pd
import numpy as np
import re
from utils.adaptive_logic import (
    choose_imputation_method,
    choose_resample_strategy,
    choose_scaling_method
)

def clean_numeric_column(series):
    """
    Vyƒçist√≠ a p≈ôevede sloupec na numerick√© hodnoty.
    Odstran√≠ nap≈ô. koment√°≈ôe v z√°vork√°ch ("81 (expected)") a p≈ôevede na float.
    """
    def parse(val):
        if isinstance(val, str):
            val = re.sub(r"\s*\(.*?\)", "", val)
        try:
            return float(val)
        except (ValueError, TypeError):
            return np.nan
    return series.apply(parse)

def preprocess_sensor_data(df_sensor, impute=True):
    """
    Plnƒõ adaptivn√≠ p≈ôedzpracov√°n√≠: strategie resamplingu, imputace a ≈°k√°lov√°n√≠ na z√°kladƒõ dat.

    Args:
        df_sensor (DataFrame): surov√° data se sloupci ['timestamp', 'object_id', 'kpi_id', 'value']
        impute (bool): zda dopl≈àovat chybƒõj√≠c√≠ hodnoty

    Returns:
        Dict[object_id, DataFrame]: p≈ôedzpracovan√° data pro ka≈æd√Ω objekt zvl√°≈°≈•
    """
    df_sensor['timestamp'] = pd.to_datetime(df_sensor['timestamp'])
    df_sensor['value'] = clean_numeric_column(df_sensor['value'])

    required_cols = {'timestamp', 'object_id', 'kpi_id', 'value'}
    missing_cols = required_cols - set(df_sensor.columns)
    if missing_cols:
        raise ValueError(f"‚ùå Chyb√≠ sloupce: {missing_cols}")

    df_pivot = pivot_sensor_data(df_sensor)

    processed_data = {}
    for machine_id, df_machine in df_pivot.groupby(level=0):
        df_machine = df_machine.copy()
        print(f"üõ†Ô∏è Zpracov√°v√°m objekt {machine_id}")

        # Z√≠sk√°n√≠ strategie pro ka≈æd√Ω KPI (freq, agg)
        resample_strategies = choose_resample_strategy(
            df_sensor[df_sensor['object_id'] == machine_id]
        )

        # 1) Odstran√≠me √∫rove≈à object_id z indexu:
        df_machine = df_machine.reset_index(level='object_id', drop=True)

        # 2) Ujist√≠me se, ≈æe index nese jm√©no 'timestamp'
        df_machine.index.name = 'timestamp'

        # 3) Pokud pot≈ôebujete sloupec object_id pro dal≈°√≠ zpracov√°n√≠:
        df_machine['object_id'] = machine_id


        global_start = df_machine.index.min()
        global_end = df_machine.index.max()
        start = global_start.floor('min')
        end   = global_end.ceil('min')
        df_resampled = pd.DataFrame(index=pd.date_range(start, end, freq='min'))

        for kpi_id in df_machine.columns:
            if kpi_id == 'object_id':
                continue

            strategy = resample_strategies.get(kpi_id, {'freq': '1h', 'agg': 'mean'})
            print(f"üìè KPI {kpi_id}: freq={strategy['freq']}, agg={strategy['agg']}")

            series = df_machine[kpi_id].resample(strategy['freq']).agg(strategy['agg'])

            if impute:
                method = choose_imputation_method(series)
                series = handle_missing_values(series, method)

            method = choose_scaling_method(series)
            series = normalize_sensor_data(series, method)

            df_resampled[kpi_id] = series

        df_final = generate_features(df_resampled)
        processed_data[machine_id] = df_final

    return processed_data

def handle_missing_values(series, method='ffill'):
    """Zpracuje chybƒõj√≠c√≠ hodnoty flexibilnƒõ podle zvolen√© metody."""
    if method == 'none':
        return series
    elif method == 'ffill':
        return series.ffill()
    elif method == 'bfill':
        return series.bfill()
    elif method == 'mean':
        return series.fillna(series.mean())
    elif method == 'interpolate':
        return series.interpolate()
    else:
        return series.fillna(method='ffill')

def normalize_sensor_data(series, method='zscore'):
    """Standardizuje data (Z-score, MinMax, Robust...) podle zvolen√© metody."""
    if method == 'none':
        return series

    elif method == 'zscore':
        mean = series.mean()
        std = series.std()
        # Pokud je rozptyl nulov√Ω nebo NaN, vr√°t√≠me konstantn√≠ ≈ôadu (nap≈ô. nulovou)
        if std == 0 or np.isnan(std):
            return series - mean  # nebo: return series.fillna(0)
        return (series - mean) / std

    elif method == 'minmax':
        min_ = series.min()
        max_ = series.max()
        span = max_ - min_
        if span == 0 or np.isnan(span):
            return series - min_
        return (series - min_) / span

    elif method == 'robust':
        median = series.median()
        iqr = series.quantile(0.75) - series.quantile(0.25)
        if iqr == 0 or np.isnan(iqr):
            return series - median
        return (series - median) / iqr

    else:
        # fallback na ≈æ√°dnou zmƒõnu
        return series

def pivot_sensor_data(df_sensor):
    """
    P≈ôevede data do wide-formy (ƒças √ó KPI √ó stroj) pro ML.

    P≈ôedpokl√°d√° sloupce: ['timestamp', 'object_id', 'kpi_id', 'value']
    """
    df = df_sensor.copy()
    df['timestamp'] = pd.to_datetime(df['timestamp'])
    df.set_index(['object_id', 'timestamp', 'kpi_id'], inplace=True)
    df = df.unstack('kpi_id')['value']
    return df

def generate_features(df, window_sizes=[3, 6, 12]):
    """
    Vypoƒç√≠t√° odvozen√© ƒçasov√© charakteristiky (rolling mean, std...).

    Args:
        df (DataFrame): DataFrame s resamplovan√Ωmi daty.
        window_sizes (List[int]): Velikosti oken pro rolling metriky.

    Returns:
        DataFrame: Data s p≈ôidan√Ωmi featurami.
    """
    df_feat = df.copy()
    for window in window_sizes:
        for col in df.columns:
            df_feat[f'{col}_mean_{window}'] = df[col].rolling(window).mean()
            df_feat[f'{col}_std_{window}'] = df[col].rolling(window).std()
    return df_feat

```


---

### `utils\preprocessing2.py`

```python
import pandas as pd
import numpy as np
import re
from utils.adaptive_logic import choose_scaling_method

def preprocess_sensor_data2(df, freq='1min', impute=True, windows=[3,6,12]):
    """
    Adaptivn√≠ p≈ôedzpracov√°n√≠ senzoru pro v√≠ce objekt≈Ø.

    Args:
        df (pd.DataFrame): DataFrame se sloupci ['timestamp','object_id','kpi_id','value']
        freq (str): frekvence resamplingu (nap≈ô. '1min')
        impute (bool): zda imputovat chybƒõj√≠c√≠ hodnoty
        windows (List[int]): velikosti oken pro rolling statistiky

    Returns:
        dict: { object_id: DataFrame } s p≈ôedzpracovan√Ωmi daty (featuremi) pro ka≈æd√Ω objekt
    """

    # --- 1) Rename & type casting ---
    df2 = df.rename(columns={
        'data_timestamp': 'timestamp',
        'id_fc_object': 'object_id',
        'id_fc_kpi_definition': 'kpi_id',
        'value': 'value'
    }).copy()
    df2['timestamp'] = pd.to_datetime(df2['timestamp'])
    df2['value'] = _clean_numeric_column(df2['value'])

    processed_data = {}
    # --- 2) Zpracov√°n√≠ pro ka≈æd√Ω objekt ---
    for obj_id, df_obj in df2.groupby('object_id'):
        if df_obj['kpi_id'].nunique() == 0:
            print(f"‚ö†Ô∏è Objekt {obj_id} nem√° ≈æ√°dn√© KPI data.")
            continue
        # Pivot do wide formy
        df_wide = df_obj.set_index(['timestamp', 'kpi_id'])['value'] \
                        .unstack('kpi_id') \
                        .sort_index()

        # --- 3) Resample ---
        df_res = df_wide.resample(freq).asfreq()

        # --- 4) Impute ---
        if impute:
            df_res = df_res.interpolate(method='time', limit_direction='both')
            df_res = df_res.ffill().bfill()

        # --- 5) Drop konstantn√≠ KPI ---
        variances = df_res.var()
        zero_var = variances[variances == 0].index.tolist()
        if zero_var:
            print(f"‚ö†Ô∏è Dropping constant KPIs for object {obj_id}: {zero_var}")
            df_res = df_res.drop(columns=zero_var)

        # --- 6) Scale ---
        df_scaled = pd.DataFrame(index=df_res.index)
        for col in df_res.columns:
            method = choose_scaling_method(df_res[col])
            if method == 'none':
                df_scaled[col] = df_res[col]
            else:
                df_scaled[col] = _normalize(df_res[col], method)

        # --- 7) Feature engineering (optimalizov√°no) ---
        feature_blocks = [df_scaled]  # z√°kladn√≠ ≈°k√°lovan√° data

        # 7a) Rolling mean/std
        for w in windows:
            rolling_means = df_scaled.rolling(w, min_periods=1).mean().add_suffix(f"_mean_{w}")
            rolling_stds = df_scaled.rolling(w, min_periods=1).std().add_suffix(f"_std_{w}")
            feature_blocks.extend([rolling_means, rolling_stds])

        # 7b) Diference a procentu√°ln√≠ zmƒõna
        diffs1 = df_scaled.diff(1).fillna(0).add_suffix("_diff1")
        diffs2 = df_scaled.diff(2).fillna(0).add_suffix("_diff2")
        pct_changes = df_scaled.pct_change().fillna(0).add_suffix("_pct_change")
        feature_blocks.extend([diffs1, diffs2, pct_changes])

        # 7c) ƒåas od posledn√≠ho mƒõ≈ôen√≠ (proxy pomoc√≠ prvn√≠ KPI)
        last_obs = df_wide.notna().cumsum(axis=0)
        time_since = (last_obs != last_obs.shift(1)).cumsum(axis=0)
        if time_since.shape[1] > 0:
            time_since_last = time_since.iloc[:, 0].rename("time_since_last")
        else:
            time_since_last = pd.Series(0, index=df_scaled.index, name="time_since_last")
        feature_blocks.append(time_since_last)

        # Fin√°ln√≠ spojen√≠ featur
        df_feat = pd.concat(feature_blocks, axis=1)

        processed_data[obj_id] = df_feat

    return processed_data


def _clean_numeric_column(series):
    def parse(x):
        if isinstance(x, str):
            x = re.sub(r"\s*\(.*?\)", "", x)
        try:
            return float(x)
        except:
            return np.nan
    return series.apply(parse)


def _normalize(s, method):
    m, M = s.min(), s.max()
    if method == 'zscore':
        mu, sigma = s.mean(), s.std()
        return (s - mu) / sigma if sigma > 0 else s - mu
    if method == 'minmax':
        span = M - m
        return (s - m) / span if span > 0 else s - m
    if method == 'robust':
        med = s.median()
        iqr = s.quantile(.75) - s.quantile(.25)
        return (s - med) / iqr if iqr > 0 else s - med
    return s

```


---

### `utils\visualization.py`

```python
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np

# 1. Trend vsech KPI v jednom grafu

def plot_kpi_raw_trends(df, object_id=None, kpi_ids=None, feature_types=None, agg_freq='1H', highlight_missing=False,
                        start_time=None, end_time=None):
    if isinstance(df, dict) and object_id is not None:
        df = df.get(object_id, pd.DataFrame())

    if not isinstance(df, pd.DataFrame) or df.empty:
        print("\u26a0\ufe0f Nevalidni nebo prazdna data pro vykresleni.")
        return

    df = df.resample(agg_freq).mean().dropna(how='all')

    if start_time or end_time:
        df = df.loc[start_time:end_time]

    feature_types = feature_types or ['raw', 'mean', 'std', 'diff1', 'diff2', 'pct_change', 'time_since', 'rolling']

    available_cols = df.select_dtypes(include=[np.number]).columns
    selected = []

    for col in available_cols:
        col_str = str(col)
        if 'raw' in feature_types and col_str.isdigit():
            selected.append(col)
        elif 'rolling' in feature_types and any(x in col_str for x in ['mean_', 'std_']):
            selected.append(col)
        elif any(ftype in col_str for ftype in feature_types if ftype not in ['raw', 'rolling']):
            selected.append(col)

    if kpi_ids:
        kpi_ids_str = [str(k) for k in kpi_ids]
        selected = [col for col in selected if any(k in str(col) for k in kpi_ids_str)]

    if not selected:
        print("\u26a0\ufe0f Zadny KPI odpovidajici vyberu.")
        return

    plt.figure(figsize=(14, 6))
    for col in selected:
        series = df[col]
        plt.plot(series.index, series, label=col)
        if highlight_missing:
            nan_mask = series.isna()
            plt.plot(series.index[nan_mask], [np.nan]*nan_mask.sum(), 'rx', alpha=0.2)

    plt.title(f"KPI Trends (object {object_id})")
    plt.xlabel("Time")
    plt.ylabel("Value")
    plt.legend()
    plt.tight_layout()
    plt.show()


# 2. Rolling trend - samostatne grafy

def plot_kpi_rolling(df, object_id=None, kpi_ids=None, window=6, method='mean',
                     start_time=None, end_time=None):
    if isinstance(df, dict) and object_id is not None:
        df = df.get(object_id, pd.DataFrame())

    if not isinstance(df, pd.DataFrame) or df.empty:
        print("\u26a0\ufe0f Nevalidni nebo prazdna data.")
        return

    if start_time or end_time:
        df = df.loc[start_time:end_time]

    available = df.select_dtypes(include=[np.number]).columns
    selected = kpi_ids if kpi_ids is not None else available
    selected = [col for col in selected if col in available]

    for col in selected:
        series = df[col]
        roll_series = getattr(series.rolling(window=window, min_periods=1), method)()

        plt.figure(figsize=(12, 5))
        plt.plot(df.index, series, label=f'{col} (raw)', alpha=0.5)
        plt.plot(df.index, roll_series, label=f'{col} ({method}, w={window})', linewidth=2)
        plt.title(f"Rolling {method} of KPI {col} (object {object_id})")
        plt.xlabel("Time")
        plt.ylabel("Value")
        plt.legend()
        plt.tight_layout()
        plt.show()


# 3. Korelacni matice

def plot_correlation_heatmap(df, object_id=None, kpi_ids=None, feature_types=None,
                             start_time=None, end_time=None):
    if isinstance(df, dict) and object_id is not None:
        df = df.get(object_id, pd.DataFrame())

    if not isinstance(df, pd.DataFrame) or df.empty:
        print("\u26a0\ufe0f Nevalidni nebo prazdna data.")
        return

    if start_time or end_time:
        df = df.loc[start_time:end_time]

    df_numeric = df.select_dtypes(include=[np.number])
    if df_numeric.empty:
        print("\u26a0\ufe0f Zadna ciselna data k dispozici.")
        return

    feature_types = feature_types or ['raw', 'mean', 'std', 'diff1', 'diff2', 'pct_change', 'time_since']
    selected_cols = []

    for col in df_numeric.columns:
        col_str = str(col)
        if 'raw' in feature_types and col_str.isdigit():
            selected_cols.append(col)
        elif 'rolling' in feature_types and any(x in col_str for x in ['mean_', 'std_']):
            selected_cols.append(col)
        elif any(ftype in col_str for ftype in feature_types if ftype not in ['raw', 'rolling']):
            selected_cols.append(col)

    if kpi_ids:
        kpi_ids_str = [str(k) for k in kpi_ids]
        selected_cols = [col for col in selected_cols if any(k in str(col) for k in kpi_ids_str)]

    if len(selected_cols) < 2:
        print("\u26a0\ufe0f Nedostatecny pocet sloupcu pro korelaci.")
        return

    corr = df_numeric[selected_cols].corr()
    plt.figure(figsize=(10, 8))
    sns.heatmap(corr, cmap="coolwarm", center=0, annot=False)
    plt.title(f"Correlation heatmap for object {object_id}")
    plt.tight_layout()
    plt.show()


# 4. Histogramy

def plot_feature_distributions(df, object_id=None, kpi_ids=None, raw=False, log=False,
                               start_time=None, end_time=None):
    if isinstance(df, dict) and object_id is not None:
        df = df.get(object_id, pd.DataFrame())

    if not isinstance(df, pd.DataFrame) or df.empty:
        print("\u26a0\ufe0f Nevalidni nebo prazdna data.")
        return

    if start_time or end_time:
        df = df.loc[start_time:end_time]

    df_plot = df.copy()
    if raw:
        df_plot = df_plot[[col for col in df_plot.columns if str(col).isdigit()]]

    if kpi_ids:
        df_plot = df_plot[[col for col in kpi_ids if col in df_plot.columns]]

    selected = df_plot.select_dtypes(include=[np.number]).columns
    if selected.empty:
        print("\u26a0\ufe0f Zadne ciselne sloupce.")
        return

    for col in selected:
        series = df_plot[col].replace([np.inf, -np.inf], np.nan).dropna()
        if series.empty:
            print(f"\u26a0\ufe0f Sloupec {col} obsahuje pouze NaN/inf ‚Äì preskoceno.")
            continue

        plt.figure(figsize=(8, 4))
        series.hist(bins=30, log=log)
        plt.title(f"Histogram of {col} {'(raw)' if raw else ''}")
        plt.xlabel("Value")
        plt.ylabel("Frequency")
        plt.tight_layout()
        plt.show()


# 5. Missing data pattern

def plot_missing_data_pattern(df, object_id=None, n_rows=500,
                              start_time=None, end_time=None):
    if isinstance(df, dict) and object_id is not None:
        df = df.get(object_id, pd.DataFrame())

    if not isinstance(df, pd.DataFrame) or df.empty:
        print("\u26a0\ufe0f Nevalidni nebo prazdna data.")
        return

    if start_time or end_time:
        df = df.loc[start_time:end_time]

    df_cut = df.iloc[:n_rows].isna()
    if df_cut.empty:
        print("\u26a0\ufe0f Zadne data pro vykresleni chyb.")
        return

    plt.figure(figsize=(12, 6))
    sns.heatmap(df_cut.T, cbar=False, cmap='viridis', xticklabels=False)
    plt.title("Missing data pattern (first rows)")
    plt.xlabel("Time steps")
    plt.ylabel("Features")
    plt.tight_layout()
    plt.show()

```
